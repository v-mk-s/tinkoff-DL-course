{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"o13_1 Reinforce.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Выводы** в конце ноутбука"],"metadata":{"id":"ab6S6sEoIka-"}},{"cell_type":"markdown","metadata":{"id":"VbCJU6pY27HZ"},"source":["# REINFORCE\n","\n","Пусть $X$ это какая-то случайная величина с известным распределением $p_\\theta(X)$, где $\\theta$ — это параметры модели. Определим функцию награды $J(\\theta)$ как\n","\n","$$ J(\\theta) = E[f(X)] = \\int_x f(x) p(x) \\; dx $$\n","\n","для произвольной функции $f$, не зависящей от $\\theta$.\n","\n","*Policy gradient* (дословно, градиент стратегии) — это, интуитивно, то направление, куда нужно двигать параметры модели, чтобы функция награды увеличивалась. Алгоритм REINFORCE (почему его всегда пишут капсом автор не знает) на каждом шаге просто определяет policy gradient $\\nabla_\\theta J(\\theta)$ и изменяет параметры в его сторону с каким-то learning rate-ом.\n","\n","**Зачем это надо**, если есть обычный градиентный спуск? Через policy gradient и reinforcement learning вообще можно оптимизировать более общий класс функций — хоть дискретные (например, можно BLEU для перевода напрямую максимизировать) и даже невычислимые (какие-нибудь субъективные оценки асессоров).\n","\n","В частности, в таком ключе можно описать игры (для простоты, однопользовательские): есть какие-то награды за совершение каких-то действий (для шахмат: +1 за победу, 0 за ничью, -1 за поражение; для тетриса: +0.1 за «выживание» ещё одну секунду, 1 за удаление слоя, 0 за проигрыш) и нам нужно подобрать такие параметры модели, чтобы максимизировать ожидаемую сумму наград за действия, которые мы совершили, то есть в точности $J(\\theta)$.\n","\n","Теперь немного математики: как нам найти этот $\\nabla_\\theta J(\\theta)$? Оказывается, мы можем выразить его ожидание, а тогда приблизительный градиент можно будет находить сэмплированием и усреднением градиентов — так же, как мы обычно обучаем нейросети на батчах.\n","\n","$$ \\nabla J(\\theta) = \\nabla E[f(X)] = \\int_x f(x) \\nabla p(x) \\; dx = \\int_x f(x) p(x) \\nabla_\\theta \\log p(x) \\; dx = E[f(x) \\nabla_\\theta \\log p(x)] $$\n","\n","Переход между 2 и 3 верен, потому что $\\nabla \\log p(x) = \\frac{\\nabla p(x)}{p(x)}$ (просто подставьте и $p(x)$ сократится). Это называют log-derivative trick.\n","\n","Мы научились получить приблизительный градиент через сэмплирование. Давайте теперь что-нибудь обучим."]},{"cell_type":"markdown","metadata":{"id":"49kATdp-27Hc"},"source":["## `CartPole-v0`"]},{"cell_type":"markdown","metadata":{"id":"1rY14MXK27Hd"},"source":["Про задачу можно почитать тут: https://gym.openai.com/envs/CartPole-v0/. Tl;dr: есть вертикально стоящая палка на подвижной платформе; нужно двигать платформу так, чтобы палка не упала.\n","\n","<img width='500px' src='https://preview.redd.it/sqjzj2cgnpt21.gif?overlay-align=bottom,left&overlay-pad=8,16&crop=1200:628.272251309,smart&overlay-height=0.10&overlay=%2Fv9vyirk6hl221.png%3Fs%3Db466421949eb723078743745ce6421609d7a9c66&width=1200&height=628.272251309&s=ba84ac5a9c14946456808c15f2754cb7369b8de9'>\n","\n","OpenAI в 2016-м году выпустили `gym` — библиотечку для абстрагирования RL-ных сред от алгоритма. Есть абстрактная *среда* (`env`), в ней есть какие-то *состояния* (`state`), из каждого состояния есть какой-то фиксированный набо *действий* (`action`), ведущих (возможно, с какими-то вероятностями) в другие состояния, и за разные действия в разных состояниях дается какая-то *награда* (`reward`). Как конкретно устроена игра, нам думать не нужно."]},{"cell_type":"code","metadata":{"id":"YbExof5D27Hf","executionInfo":{"status":"ok","timestamp":1644533546785,"user_tz":-180,"elapsed":285,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}}},"source":["import os\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n","    !bash ../xvfb start\n","    %env DISPLAY=:1"],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"id":"a32D48N427Hl","executionInfo":{"status":"ok","timestamp":1644533547162,"user_tz":-180,"elapsed":28,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}}},"source":["import gym\n","import numpy as np, pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","env = gym.make(\"CartPole-v0\").env\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","#plt.imshow(env.render(\"rgb_array\"))\n","env.close()"],"execution_count":92,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4tM1-JRK0Nz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6e5b3da1-498c-4fce-bf58-e17dfda84521","executionInfo":{"status":"ok","timestamp":1644533547163,"user_tz":-180,"elapsed":28,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}}},"source":["state_dim # 4 состояния системы:"],"execution_count":93,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4,)"]},"metadata":{},"execution_count":93}]},{"cell_type":"code","source":["n_actions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pfR6kQA1WxVM","executionInfo":{"status":"ok","timestamp":1644533547165,"user_tz":-180,"elapsed":28,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}},"outputId":"0e061bab-c530-4e7a-9948-76abf879a7a0"},"execution_count":94,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":94}]},{"cell_type":"markdown","metadata":{"id":"MHbZLha-27Hr"},"source":["## Сеть\n","\n","Для REINFORCE вам нужна модель, которая берёт на вход состояние (каким-то образом закодированное) и возвращает вероятностное распределение действий в нём.\n","\n","Старайтесь не перемудрить — в общем случае сети для RL могут быть [довольно сложными](https://d4mucfpksywv.cloudfront.net/research-covers/openai-five/network-architecture.pdf), но CartPole не стоит того, чтобы писать глубокие архитектуры."]},{"cell_type":"code","metadata":{"id":"oynCK8dd27Ht","executionInfo":{"status":"ok","timestamp":1644533547171,"user_tz":-180,"elapsed":30,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}}},"source":["import torch\n","import torch.nn as nn"],"execution_count":95,"outputs":[]},{"cell_type":"code","metadata":{"id":"f48sXTXa27Hv","executionInfo":{"status":"ok","timestamp":1644533547172,"user_tz":-180,"elapsed":30,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}}},"source":["agent = nn.Sequential(\n","    nn.Linear(4, 32),\n","    nn.Sigmoid(),\n","    nn.Linear(32, 2),\n","    nn.LogSoftmax(dim=1)  # важно, что на выходе должны быть не вероятности, а логиты\n",")"],"execution_count":96,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ggyG8ils27Hx"},"source":["OpenAI Gym работает с numpy, а не напрямую с фреймворками. Для удобства, напишите функцию-обёртку, которая принимает состояния (`numpy array` размера `[batch, state_shape]`) и возвращает вероятности (размера `[batch, n_actions]]`, должны суммироваться в единицу)."]},{"cell_type":"code","metadata":{"id":"JXJ3H3LS27Hy","executionInfo":{"status":"ok","timestamp":1644533547173,"user_tz":-180,"elapsed":30,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}}},"source":["def predict_proba(states):\n","    states = torch.Tensor(states)\n","    logits = agent(states)\n","    # сконвертируйте состояния в тензор\n","    # вычислите логиты\n","    # вызовите софтмакс, чтобы получить веряотности\n","    return logits.exp().detach().numpy() # не bottleneck, поэтому так можно написать, но лучше torch.exp(logits...)"],"execution_count":97,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZnYJWRK27H0","executionInfo":{"status":"ok","timestamp":1644533547174,"user_tz":-180,"elapsed":31,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}}},"source":["test_states = np.array([env.reset() for _ in range(5)])\n","test_probas = predict_proba(test_states)\n","assert isinstance(test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n","assert tuple(test_probas.shape) == (test_states.shape[0], n_actions), \"wrong output shape: %s\" % np.shape(test_probas)\n","assert np.allclose(np.sum(test_probas, axis = 1), 1), \"probabilities do not sum to 1\""],"execution_count":98,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fRoxP08z27H1"},"source":["## Тестовый прогон\n","\n","Хоть наша модель не обучена, её уже можно использовать, чтобы играть в произвольной среде."]},{"cell_type":"code","metadata":{"id":"hoM9YW5H27H2","executionInfo":{"status":"ok","timestamp":1644533547175,"user_tz":-180,"elapsed":31,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}}},"source":["def generate_session(t_max=1000):\n","    \"\"\" \n","    Играет одну сессию REINFORCE-агентом и возвращает последовательность состояний,\n","    действий и наград, которые потом будут использоваться при обучении.\n","    \"\"\"\n","    \n","    # тут будем хранить сессию\n","    states, actions, rewards = [],[],[]\n","    \n","    s = env.reset()\n","    \n","    for t in range(t_max):\n","        \n","        # вероятности следующих действий, aka p(a|s)\n","        action_probas = predict_proba(np.array([s]))[0] \n","        \n","        # сэмплируйте оттуда действие (посказка: np.random.choice)\n","        a = np.random.choice([0, 1], p=action_probas)\n","        \n","        new_s, r, done, info = env.step(a) # info здесь не используется\n","        \n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","        \n","        s = new_s\n","        if done:\n","            break\n","            \n","    return states, actions, rewards"],"execution_count":99,"outputs":[]},{"cell_type":"code","metadata":{"id":"RChXl6GD27H3","executionInfo":{"status":"ok","timestamp":1644533547176,"user_tz":-180,"elapsed":31,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}}},"source":["# протестируйте\n","states, actions, rewards = generate_session()"],"execution_count":100,"outputs":[]},{"cell_type":"code","source":["states"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ViLlX2vIfd-p","executionInfo":{"status":"ok","timestamp":1644533547177,"user_tz":-180,"elapsed":32,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}},"outputId":"b9a0fd1d-ae13-438d-f190-1e73e0bb6178"},"execution_count":101,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([ 0.0181061 , -0.04347813,  0.02581502, -0.00647325]),\n"," array([ 0.01723654,  0.15126427,  0.02568556, -0.29090075]),\n"," array([ 0.02026182, -0.04421434,  0.01986754,  0.00977116]),\n"," array([ 0.01937753, -0.2396155 ,  0.02006296,  0.30865573]),\n"," array([ 0.01458522, -0.04478508,  0.02623608,  0.02236703]),\n"," array([ 0.01368952,  0.14995099,  0.02668342, -0.26192403]),\n"," array([ 0.01668854, -0.04554149,  0.02144494,  0.0390543 ]),\n"," array([ 0.01577771,  0.14926648,  0.02222603, -0.24678614]),\n"," array([ 0.01876304, -0.04616573,  0.0172903 ,  0.0528237 ]),\n"," array([ 0.01783973,  0.14870408,  0.01834678, -0.23435424]),\n"," array([ 0.02081381, -0.04667513,  0.01365969,  0.06405891]),\n"," array([ 0.01988031,  0.14824833,  0.01494087, -0.22428319]),\n"," array([ 0.02284527,  0.34315359,  0.01045521, -0.51221599]),\n"," array([ 2.97083454e-02,  5.38126730e-01,  2.10886368e-04, -8.01585939e-01]),\n"," array([ 0.04047088,  0.34300189, -0.01582083, -0.50883668]),\n"," array([ 0.04733092,  0.53834312, -0.02599757, -0.80646299]),\n"," array([ 0.05809778,  0.73381159, -0.04212683, -1.10720904]),\n"," array([ 0.07277401,  0.92946123, -0.06427101, -1.41280475]),\n"," array([ 0.09136324,  0.73519204, -0.0925271 , -1.1408853 ]),\n"," array([ 0.10606708,  0.93139352, -0.11534481, -1.46109278]),\n"," array([ 0.12469495,  1.12772507, -0.14456666, -1.78746945]),\n"," array([ 0.14724945,  1.32414421, -0.18031605, -2.12137942])]"]},"metadata":{},"execution_count":101}]},{"cell_type":"code","source":["actions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lBT1t6rIfezC","executionInfo":{"status":"ok","timestamp":1644533547552,"user_tz":-180,"elapsed":402,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}},"outputId":"d6b15cf5-5fb8-40aa-ffe7-e2a88c3ca6f8"},"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0]"]},"metadata":{},"execution_count":102}]},{"cell_type":"code","source":["rewards"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HbBznYOuffgu","executionInfo":{"status":"ok","timestamp":1644533547553,"user_tz":-180,"elapsed":20,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}},"outputId":"0c1c773e-0c7b-4311-c7f9-9abaa43246fb"},"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0]"]},"metadata":{},"execution_count":103}]},{"cell_type":"markdown","metadata":{"id":"d0xhuQgh27H5"},"source":["### Computing cumulative rewards"]},{"cell_type":"code","metadata":{"id":"DOF_VZIv27H5","executionInfo":{"status":"ok","timestamp":1644533547555,"user_tz":-180,"elapsed":17,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}}},"source":["def get_cumulative_rewards(rewards, gamma=0.99):\n","    \"\"\"\n","    Принимает массив ревардов и возвращает discounted массив по следующей формуле:\n","    \n","        G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","    \n","    Тут нет ничего сложного -- итерируйтесь от последнего до первого\n","    и насчитывайте G_t = r_t + gamma*G_{t+1} рекуррентно.\n","    \"\"\"\n","    \n","    g = 0\n","    g_t = []\n","    for r in reversed(rewards):\n","      g = r + gamma * g\n","      g_t.append(g)\n","    return np.array(list(reversed(g_t)))"],"execution_count":104,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4H6MNAo27H7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644533547557,"user_tz":-180,"elapsed":18,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}},"outputId":"b38c634d-2564-42fa-da5c-6aa61e5bfac5"},"source":["get_cumulative_rewards(rewards)\n","assert len(get_cumulative_rewards(list(range(100)))) == 100\n","assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n","                   [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n","                   [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n","                   [0, 0, 1, 2, 3, 4, 0])\n","print(\"Вроде норм\")"],"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["Вроде норм\n"]}]},{"cell_type":"markdown","metadata":{"id":"24NHQ2_n27H8"},"source":["#### Loss function and updates\n","\n","Вспомним, что нам нужно оптимизировать\n","\n","$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n","\n","\n","Используя REINFORCE, нам в алгоритме по сути нужно максимизировать немного другую функцию:\n","\n","$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n","\n","Когда мы будем вычислять её градиент, мы получим в точности policy gradient из REINFORCE."]},{"cell_type":"code","metadata":{"id":"63Vhul4D27H8","executionInfo":{"status":"ok","timestamp":1644533547558,"user_tz":-180,"elapsed":15,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}}},"source":["def to_one_hot(y_tensor, n_dims=None):\n","    \"\"\" Конвертирует целочисленный вектор в one-hot матрицу. \"\"\"\n","    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n","    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n","    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n","    return y_one_hot"],"execution_count":106,"outputs":[]},{"cell_type":"code","metadata":{"id":"BsBAlZID27H9","executionInfo":{"status":"ok","timestamp":1644533547559,"user_tz":-180,"elapsed":15,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}}},"source":["optimizer = torch.optim.Adam(agent.parameters())\n","# тут определите оптимизатор для модели\n","# например, Adam с дефолтными параметрами\n","\n","# натрениться с конкретной функции\n","def train_on_session(states, actions, rewards, gamma = 0.99):\n","    optimizer.zero_grad()\n","\n","    states = torch.tensor(states, dtype=torch.float32)\n","    actions = torch.tensor(actions, dtype=torch.int32)\n","    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n","    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n","    \n","    logprobas = agent(states)\n","    probas = logprobas.exp()\n","    \n","    assert all(isinstance(v, torch.Tensor) for v in [probas, logprobas]), \\\n","        \"please use compute using torch tensors and don't use predict_proba function\"\n","    \n","    # выберем и просуммируем лог-вероятности только для выбранных действий\n","    # аналог кросс-энтропии\n","    logprobas_for_actions = torch.sum(logprobas * to_one_hot(actions), dim=1)\n","    \n","    J_hat = torch.mean(cumulative_returns * logprobas_for_actions) # берем среднее от итераций, которые совершали # формула для REINFORCE\n","    \n","\n","    # опционально: энтропийная регуляризация\n","    # не регуляризация, а способ не скатываться в локальный минимум\n","    entropy_reg = torch.mean(logprobas * probas) # вычислите среднюю энтропию вероятностей; не забудьте знак!\n","    \n","    loss = - J_hat - 0.1 * entropy_reg\n","    \n","    loss.backward()\n","    optimizer.step()\n","    # шагните в сторону градиента\n","    # ....\n","    \n","    # верните ревард сессии, чтобы потом их печатать\n","    return np.sum(rewards)"],"execution_count":107,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"spOFhZuG27H-"},"source":["## Само обучение"]},{"cell_type":"code","metadata":{"id":"tp6P9a6-27H-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644533692999,"user_tz":-180,"elapsed":145454,"user":{"displayName":"Vladislav Melnichuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3F7nSpPnFoXxL_fPnQxVBeJp7KojsNed_OL2toQ=s64","userId":"12987108233979345356"}},"outputId":"fbd9acd7-6788-492d-b4c8-ea6d853264e3"},"source":["# играют на батче независимых сессий\n","\n","for i in range(100):\n","    \n","    rewards = [train_on_session(*generate_session()) for _ in range(100)]\n","    \n","    print (i, \" mean reward:%.3f\"%(np.mean(rewards)))\n","\n","    if np.mean(rewards) > 500:\n","        print (\"Победа!\")\n","        break\n","\n","# выводит количество тиков, которое мы продержались"],"execution_count":108,"outputs":[{"output_type":"stream","name":"stdout","text":["0  mean reward:21.440\n","1  mean reward:19.620\n","2  mean reward:23.910\n","3  mean reward:20.920\n","4  mean reward:18.170\n","5  mean reward:24.000\n","6  mean reward:23.970\n","7  mean reward:23.350\n","8  mean reward:25.150\n","9  mean reward:22.790\n","10  mean reward:27.130\n","11  mean reward:29.330\n","12  mean reward:29.760\n","13  mean reward:33.150\n","14  mean reward:32.420\n","15  mean reward:32.160\n","16  mean reward:33.410\n","17  mean reward:39.270\n","18  mean reward:36.510\n","19  mean reward:43.410\n","20  mean reward:40.690\n","21  mean reward:47.880\n","22  mean reward:47.470\n","23  mean reward:54.420\n","24  mean reward:46.970\n","25  mean reward:52.620\n","26  mean reward:68.330\n","27  mean reward:64.650\n","28  mean reward:60.230\n","29  mean reward:62.810\n","30  mean reward:82.920\n","31  mean reward:100.550\n","32  mean reward:88.540\n","33  mean reward:115.690\n","34  mean reward:149.640\n","35  mean reward:137.450\n","36  mean reward:225.920\n","37  mean reward:173.110\n","38  mean reward:207.510\n","39  mean reward:138.170\n","40  mean reward:163.730\n","41  mean reward:211.500\n","42  mean reward:276.020\n","43  mean reward:207.720\n","44  mean reward:129.030\n","45  mean reward:175.180\n","46  mean reward:137.350\n","47  mean reward:163.280\n","48  mean reward:390.280\n","49  mean reward:223.890\n","50  mean reward:248.600\n","51  mean reward:156.330\n","52  mean reward:503.530\n","Победа!\n"]}]},{"cell_type":"markdown","metadata":{"id":"BLe3kyRO27IA"},"source":["## Видосик"]},{"cell_type":"code","source":["import gym.wrappers\n","env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n","# sessions = [generate_session() for _ in range(100)]\n","sessions = []\n","for _ in range(100):\n","  states, actions, rewards = generate_session()\n","  sessions.append((states, actions, rewards))\n","env.close()"],"metadata":{"id":"tiEJcYxA5Rxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import HTML\n","import os\n","\n","video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(\"./videos/\"+video_names[-1]))"],"metadata":{"id":"d88o06Vs5S-j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Выводы:**\n","\n","1. Мы победили! 503.5 балла"],"metadata":{"id":"mbnJ3PJFIrEw"}}]}